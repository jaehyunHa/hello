T
F
xinf <- Inf
yinf <- Inf
yinf <- -Inf
xinf/yinf
sum(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
KK <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
ls()
seq(1,10)
seq(1,101)
seq(1,101, by=2)
seq(1,101, by=1)
seq(1,101, by=3)
seq(1,100, by=2, from =2)
library(KoNLP)
library("KoNLP")
setwd("~/")
library("KoNLP")
library(KoNLP)
seq(1,100, by=2, length.out=20)
seq(1,100,length.out=20)
seq(1,100,length.out=20)
seq(1,100,length.out=10)
rep(c(1,2,3), times=3))
rep(c(1,2,3), times=3)
rep(1:4, 2)
library(KoNLP)
install.packages("KoNLP")
library(KoNLP)
library(KoNLP)
library(KoNLP)
library(KoNLP)
library(KoNLP)
library(KoNLP)
library(KoNLP)
library(KoNLP)
library(KoNLP)
install.packages(“KoNLP”)
install.packages("KoNLP")
library(KoNLP)
install.packages("KoNLP")
library(KoNLP)
sessionInfo()
install.packages("KoNLP")
library(KoNLP)
install.packages("KoNLP")
remove.packages("KoNLP", lib="~/R/win-library/3.5")
install.packages("KoNLP", lib="C:/Program Files/R/R-3.5.1/library")
library(KoNLP)
.libPaths()
library(KoNLP)
install.packages("KoNLP", lib="C:/Program Files/R/R-3.5.1/library")
detach("package:KoNLP", unload = TRUE)
library(KoNLP)
detach("package:KoNLP", unload = TRUE)
install.packages("KoNLP", lib="C:/Program Files/R/R-3.5.1/library")
library(KoNLP)
.libPaths()
library(recommenderlab)
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(matrixStats)
install.packages("matrixStats")
library(matrixStats)
rating_df <- tribble(
~ user, ~슈퍼맨,  ~배트맨,  ~색계, ~아멜리에,
"병곤",     1 ,      2 ,      8 ,     10,
"용섭",     10,       7,       8,      3,
"우상",     8 ,      9 ,      9 ,     2,
"성수",     4 ,      5 ,      9 ,     7)
rating_mat <- rating_df %>%
remove_rownames() %>%
column_to_rownames(var="user") %>%
as.matrix()
rating_mat
DT::datatable(rating_mat)
library(recommenderlab)
library(tidyverse)
library(matrixStats)
DT::datatable(rating_mat)
install.packages("DT")
library(DT)
DT::datatable(rating_mat)
(rating_svd <- svd(rating_mat))
D <- diag(rating_svd$d)
D[3:4, 3:4] <- 0
(rating_approx <- rating_svd$u %*% D %*% t(rating_svd$v))
sum(rating_mat - rating_approx)
rating_mat[2,c(2,4)] <- 0
rating_mean <- rowMeans(rating_mat)
rating_std  <- rowSds(rating_mat)
rating_mat[2,c(2,4)] <- rating_mean[2] * 2
rating_std_mat <- apply(rating_mat, 1, scale)
(rating_std_svd <- svd(rating_std_mat))
D <- diag(rating_std_svd$d)
D[3:4, 3:4] <- 0
rating_std_mf <- rating_std_svd$u %*% D %*% t(rating_std_svd$v)
rating_mean + rating_std_mf * rating_std
f3= function(x){1/9*x^2}
(p3<-integrate(f3, lower=1, upper=2))
7/27
f=function(x){0.5*x*x}
(m=integrate(f, lower=0, upper=2))
f2=function(x){0.5*x^3}
(v=integrate(f2, lower=0, upper=2))
x<-c(-1, 0, 1)
p<-c(1/4, 1/2, 1/4)
sum(x*p)
mean(x*p)
sum(x^2*p)-E_x^2
(E_x<-sum(x*p))
(V_x<-sum(x^2*p)-E_x^2)
var(x*p)
var(x^2*p)
sum(x^2*p)
E_x^2
(V_x<-sum(x^2*p)-E_x^2)
f3= function(x){1/9*x^2}
(p3<-integrate(f3, lower=1, upper=2))
7/27
#3)P(1<X<2)
f3= function(x){1/9*x^2}
(p3<-integrate(f3, lower=1, upper=2))
#p150
#9.
#평균
f=function(x){0.5*x*x}
(m=integrate(f, lower=0, upper=2))
#분산
f2=function(x){0.5*x^3}
(v=integrate(f2, lower=0, upper=2))
#10.
#1)확률그래프
x<-c(-1, 0, 1)
p<-c(1/4, 1/2, 1/4)
plot(x, p, type="h", lwd=5, col="blue")
#2)평균과 분산
(E_x<-sum(x*p))
(V_x<-sum(x^2*p)-E_x^2)
# 평균 취업률
# 전체취업률 mu = 54.5
stat <- scan()
shapiro.test(stat)  #귀무가설: 정규분포를 따른다
# 평균 취업률
# 전체취업률 mu = 54.5
stat <- scan()
# 정규성검정
shapiro.test(stat)  #귀무가설: 정규분포를 따른다
# 가설검정
t.test(stat,mu=54.5)
t.test(stat,mu=54.5,alternative="greater")  #대립가설: 54.5보다 크다
# Wilcoxon 부호순위검정(정규분포 안 따를때)
wilcox.test(stat,mu=54.5,alternative="greater")
### 표본크기결정(평균)
SizeMeanCI <- function(Error, Sigma, Conf.level=0.95)
{
alpha <- (1-Conf.level)/2
(Sigma*qnorm(1-alpha)/Error)^2
}
SizeMeanCI(1.5,5,0.95)
SizeMeanTest <- function(Effect,Sigma,Alpha=0.05,Power=0.8)
{
(Sigma*(qnorm(1-Alpha/2)+qnorm(Power))/Effect)^2
}
SizeMeanTest(5,10)
### 분산
# 제품강도
x <- scan()
# 수정제곱합
css <- sum((x-mean(x))^2)  # css <- (length(x)-1)*var(x)
# 자유도
df <- length(x)-1
# 분산신뢰구간
(Sigma2CI <- c(css/qchisq(0.975,df),css/qchisq(0.025,df)))
# 표준편차신뢰구간
sqrt(Sigma2CI)
### 비율추론
# Saxony 출생기록
n <- 73380
x <- 38100 #남아
p <- x/n
# 신뢰구간
prop.test(x,n)
# 가설검정
prop.test(x,n,p=0.5,alternative="greater")
### 표본크기결정(비율)
SizePropCI <- function(Error, Conf.level=0.95)
{
alpha <- (1-Conf.level)/2
(qnorm(1-alpha)/Error)^2/4
}
SizePropCI(0.031,0.95)
SizePropTest <- function(prop1,prop2,Alpha=0.05,Power=0.8)
{
delta = prop2-prop1
term <- qnorm(1-Alpha/2)*sqrt(prop1*(1-prop1))+qnorm(Power)*sqrt(prop2*(1-prop2))
(term/delta)^2
}
SizePropTest(0.4,0.5)
setwd("C:/cyberbullying_2017")
library(MASS)
library(e1071)
library(ROCR)
tdata = read.table('cyberbullying_attitude_N.txt',header=T)
#모델링을 위해서 변수들을 파일로 가져오는 것
#함수식을 고정시키면 유동성이 없기 때문
input=read.table('input_GST.txt',header=T,sep=",")
output=read.table('output_attitude.txt',header=T,sep=",")
#예측확률 긍정과 부정 변수를 받기 위한 변수
p_output=read.table('p_output_bayes.txt',header=T,sep=",")
input_vars = c(colnames(input))
output_vars = c(colnames(output))
p_output_vars = c(colnames(p_output))
form = as.formula(paste(paste(output_vars, collapse = '+'),'~',
paste(input_vars, collapse = '+')))
ind = sample(2, nrow(tdata), replace = T, prob = c(0.5,0.5))
tr_data = tdata[ind==1,]
te_data = tdata[ind==2,]
train_data.lda = naiveBayes(form, data=tr_data)
p=predict(train_data.lda, te_data, type='raw')
#예측된 종속변수의 확률값을 긍정/부정 변수에 할당
dimnames(p)=list(NULL,c(p_output_vars))
#예측확률값의 기술통계
summary(p)
mydata=cbind(te_data,p)
mydata1=read.table('naive_bayes_cyberbullying_ROC.txt', header = T)
attach(mydata1)
#실제집단과 예측집단을 이용해 tdata의 Attitude의 추정치 예측
pr=prediction(posterior.1, te_data$Attitude)
bayes_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
bayes_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
#실제집단과 예측집단을 이용해 tdata의 Attitude의 추정치 예측
pr=prediction(posterior.1, te_data$Attitude)
bayes_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
#-1)나이브 베이지안 ROC
rm(list=ls())
tdata = read.table('cyberbullying_attitude_N.txt',header=T)
#모델링을 위해서 변수들을 파일로 가져오는 것
#함수식을 고정시키면 유동성이 없기 때문
input=read.table('input_GST.txt',header=T,sep=",")
output=read.table('output_attitude.txt',header=T,sep=",")
#예측확률 긍정과 부정 변수를 받기 위한 변수
p_output=read.table('p_output_bayes.txt',header=T,sep=",")
input_vars = c(colnames(input))
output_vars = c(colnames(output))
p_output_vars = c(colnames(p_output))
form = as.formula(paste(paste(output_vars, collapse = '+'),'~',
paste(input_vars, collapse = '+')))
ind = sample(2, nrow(tdata), replace = T, prob = c(0.5,0.5))
tr_data = tdata[ind==1,]
te_data = tdata[ind==2,]
train_data.lda = naiveBayes(form, data=tr_data)
p=predict(train_data.lda, te_data, type='raw')
#예측된 종속변수의 확률값을 긍정/부정 변수에 할당
dimnames(p)=list(NULL,c(p_output_vars))
mydata=cbind(te_data,p)
mydata1=read.table('naive_bayes_cyberbullying_ROC.txt', header = T)
#실제집단과 예측집단을 이용해 tdata의 Attitude의 추정치 예측
pr=prediction(posterior.1, te_data$Attitude)
attach(mydata1)
#실제집단과 예측집단을 이용해 tdata의 Attitude의 추정치 예측
pr=prediction(posterior.1, te_data$Attitude)
bayes_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
View(te_data)
View(mydata1)
ind = sample(2, nrow(tdata), replace = T, prob = c(0.5,0.5))
tr_data = tdata[ind==1,]
te_data = tdata[ind==2,]
write.matrix(mydata, 'naive_bayes_cyberbullying_ROC.txt')
mydata1=read.table('naive_bayes_cyberbullying_ROC.txt', header = T)
attach(mydata1)
#실제집단과 예측집단을 이용해 tdata의 Attitude의 추정치 예측
pr=prediction(posterior.1, te_data$Attitude)
tr.nnet=nnet(form, data=tr_data,size=5)
#-2)뉴럴 네트워크
library(nnet)
attach(tdata)
tr.nnet=nnet(form, data=tr_data,size=5)
p=predict(tr.nnet, te_data, type='raw')
pr = prediction(p, te_data$Attitude)
neural_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
#x축 값(fpr)을 neural_x에 할당
neural_x=unlist(attr(neural_prf, 'x.values'))
neural_y=unlist(attr(neural_prf, 'y.values'))
auc=performance(pr, measure = 'auc')
auc_neural=auc@y.values[[1]]
auc_neural
lines(neural_x, neural_y, col=2, lty=2)
#1)나이브 베이즈 분류모형 평가
rm(list=ls())
library(e1071)
tdata = read.table('cyberbullying_attitude_S.txt',header=T)
input=read.table('input_GST.txt',header=T,sep=",")
output=read.table('output_attitude.txt',header=T,sep=",")
input_vars = c(colnames(input))
output_vars = c(colnames(output))
form = as.formula(paste(paste(output_vars, collapse = '+'),'~',
paste(input_vars, collapse = '+')))
ind = sample(2, nrow(tdata), replace = T, prob = c(0.5,0.5))
tr_data = tdata[ind==1,]
te_data = tdata[ind==2,]
#모형함수(분류기) 만들기
train_data.lda = naiveBayes(form, data=tr_data)
#모형 예측
ldapred=predict(train_data.lda, te_data, type = 'class')
table(te_data$Attitude, ldapred, dnn=c("Actual","Prediction"))
perm_a=function(p1, p2, p3, p4){pr_a=(p1+p4)/sum(p1, p2, p3, p4)
return (pr_a)}
perm_a(24494, 9903, 12412, 29232)   #정확도
perm_e=function(p1, p2, p3, p4){pr_a=(p2+p3)/sum(p1, p2, p3, p4)
return (pr_a)}
perm_e(24494, 9903, 12412, 29232)   #오류율
perm_a=function(p1, p2, p3, p4){pr_a=(p1)/sum(p1, p2)
return (pr_a)}
perm_a(24494, 9903, 12412, 29232)   #민감도
perm_a=function(p1, p2, p3, p4){pr_a=(p4)/sum(p3, p4)
return (pr_a)}
perm_a(24494, 9903, 12412, 29232)   #특이도
perm_a=function(p1, p2, p3, p4){pr_a=(p1)/sum(p1, p3)
return (pr_a)}
perm_a(24494, 9903, 12412, 29232)   #정밀도
#-1)나이브 베이지안 ROC
rm(list=ls())
library(MASS)
library(e1071)
library(ROCR)
tdata = read.table('cyberbullying_attitude_N.txt',header=T)
#모델링을 위해서 변수들을 파일로 가져오는 것
#함수식을 고정시키면 유동성이 없기 때문
input=read.table('input_GST.txt',header=T,sep=",")
output=read.table('output_attitude.txt',header=T,sep=",")
#예측확률 긍정과 부정 변수를 받기 위한 변수
p_output=read.table('p_output_bayes.txt',header=T,sep=",")
input_vars = c(colnames(input))
output_vars = c(colnames(output))
p_output_vars = c(colnames(p_output))
form = as.formula(paste(paste(output_vars, collapse = '+'),'~',
paste(input_vars, collapse = '+')))
ind = sample(2, nrow(tdata), replace = T, prob = c(0.5,0.5))
tr_data = tdata[ind==1,]
te_data = tdata[ind==2,]
p=predict(train_data.lda, te_data, type='raw')
#예측된 종속변수의 확률값을 긍정/부정 변수에 할당
dimnames(p)=list(NULL,c(p_output_vars))
train_data.lda = naiveBayes(form, data=tr_data)
#예측된 종속변수의 확률값을 긍정/부정 변수에 할당
dimnames(p)=list(NULL,c(p_output_vars))
p=predict(train_data.lda, te_data, type='raw')
#예측된 종속변수의 확률값을 긍정/부정 변수에 할당
dimnames(p)=list(NULL,c(p_output_vars))
#예측확률값의 기술통계
summary(p)
mydata=cbind(te_data,p)
#attach(mydata1)
attach(mydata)
#실제집단과 예측집단을 이용해 tdata의 Attitude의 추정치 예측
pr=prediction(posterior.1, te_data$Attitude)
bayes_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
#auc 곡선의 성능 평가
auc = performance(pr, measure = 'auc')
auc_bayes=auc@y.values[[1]]
plot(bayes_prf, col=1, lty=1, lwd=1.5, main="ROC Curver for ML Models")
#ROC 곡선 기준
abline(0,1, lty=3)
#-2)뉴럴 네트워크
library(nnet)
attach(tdata)
tr.nnet=nnet(form, data=tr_data,size=5)
p=predict(tr.nnet, te_data, type='raw')
pr = prediction(p, te_data$Attitude)
neural_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
#x축 값(fpr)을 neural_x에 할당
neural_x=unlist(attr(neural_prf, 'x.values'))
neural_y=unlist(attr(neural_prf, 'y.values'))
auc=performance(pr, measure = 'auc')
auc_neural=auc@y.values[[1]]
auc_neural
lines(neural_x, neural_y, col=2, lty=2)
#-3)로지스틱 회귀
i_logistic=glm(form, family = binomial, data=tr_data)
p=predict(i_logistic, te_data, type='response')
pr = prediction(p, te_data$Attitude)
lo_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
lo_x=unlist(attr(lo_prf, 'x.values'))
lo_y=unlist(attr(lo_prf, 'y.values'))
auc=performance(pr, measure = 'auc')
auc_lo=auc@y.values[[1]]
auc_lo
lines(lo_x, lo_y, col=3, lty=3)
#-5)랜덤 포레스트
library(randomForest)
tdata.rf = randomForest(form, data=tr_data, forest=False, importance=TRUE)
p=predict(tdata.rf, te_data)
pr = prediction(p, te_data$Attitude)
ran_prf=performance(pr, measure = 'tpr', x.measure = 'fpr')
ran_x=unlist(attr(ran_prf, 'x.values'))
ran_y=unlist(attr(ran_prf, 'y.values'))
auc=performance(pr, measure = 'auc')
auc_ran=auc@y.values[[1]]
auc_ran
lines(ran_x, ran_y, col=5, lty=5)
legend('bottomright', legend=c('naive=',auc_bayes, 'neural=',auc_neural, 'logistics=', auc_lo,
'svm=', auc_svm, 'random=', auc_ran, 'decision=', auc_tree),
cex=0.7)
legend('bottomright', legend=c('naive=',auc_bayes, 'neural=',auc_neural, 'logistics=', auc_lo,
'random=', auc_ran),
cex=0.7)
##5. 시각화
#1)텍스트 데이터 -워드 클라우드
#2)시계열 데이터- 선그래프(plot), 막대그래프(barplot)
rm(list=ls())
cyber_bullying=read.table(file='긴장비행_DOV_학교폭력TXT', header=T)
cyber_bullying=read.table(file='긴장비행_DOV_학교폭력.txt', header=T)
windows(height = 8.5, width=8
)
plot(cyberbullying$tf, cyberbullying$df, xlim=c(0,11500), ylim=c(-1, 1.5), pch=18,
col=8, xlab='average_term_frequency', ylab='time_weigted_increasing_rate',
main='keyword emergency map')
plot(cyber_bullying$tf, cyber_bullying$df, xlim=c(0,11500), ylim=c(-1, 1.5), pch=18,
col=8, xlab='average_term_frequency', ylab='time_weigted_increasing_rate',
main='keyword emergency map')
cyberbullying=read.table(file='긴장비행_DOV_학교폭력.txt', header=T)
plot(cyberbullying$tf, cyberbullying$df, xlim=c(0,11500), ylim=c(-1, 1.5), pch=18,
col=8, xlab='average_term_frequency', ylab='time_weigted_increasing_rate',
main='keyword emergency map')
text(cyberbullying$tf, cyberbullying$df, labels = cyber_bullying$학교폭력, cex=0.8, col='red')
abline(h=0.206, v=712, lty=1, col=4, lwd=0.5)
#사이버 학교폭력 시간별 문서현황 시각화
rm(list=ls())
cyber=read.csv('cyberbullying_time.csv', sep="", stringsAsFactors = F)
a=cyber$X2013
b=cyber$X2014
c=cyber$X2015
d=cyber$X2016
e=cyber$total
plot(a, xlab="", ylab="", ylim=c(0,12), type="o", axes=FASLE, ann=F, col=1)
title(main = '사이버 학교폭력 시간별 문서 현황', col.main=1, font.main=2)
title(xlab = '시간', col.lab=1)
title(ylab = '버즈', col.lab=1)
axis(1, at=1:24, lab=c(cyber$시간), las=2)
axis(2, ylim=0:12, las=2)
lines(b,col=2, type="o")
lines(c,col=3, type="o")
lines(d,col=4, type="o")
lines(e,col=5, type="o")
colors=c(1,2,3,4,5)
legend(18, 1, c("2013", "2014", "2015", "2016", "total"), cex=0.9, col=colors, lty=1, lwd=2)
#사이버 학교폭력 시간별 문서현황 시각화
rm(list=ls())
cyber=read.csv('cyberbullying_time.csv', sep="", stringsAsFactors = F)
a=cyber$X2013
View(cyber)
cyber=read.csv('cyberbullying_time.csv', sep=",", stringsAsFactors = F)
a=cyber$X2013
b=cyber$X2014
c=cyber$X2015
d=cyber$X2016
e=cyber$total
#axes : x, y축 표시 안함/ann=F: x, y축 제목 표시 안함
plot(a, xlab="", ylab="", ylim=c(0,12), type="o", axes=FASLE, ann=F, col=1)
title(main = '사이버 학교폭력 시간별 문서 현황', col.main=1, font.main=2)
title(xlab = '시간', col.lab=1)
title(ylab = '버즈', col.lab=1)
#축지정위(1:x, 2:y)/ x축 범위/   /1:수평, 2:수직
axis(1, at=1:24, lab=c(cyber$시간), las=2)
axis(2, ylim=0:12, las=2)
lines(b,col=2, type="o")
lines(c,col=3, type="o")
lines(d,col=4, type="o")
lines(e,col=5, type="o")
colors=c(1,2,3,4,5)
legend(18, 1, c("2013", "2014", "2015", "2016", "total"), cex=0.9, col=colors, lty=1, lwd=2)
#3)지리적 데이터
library(sp)
gadm=readRDS("KOR_adm0.rds")
plot(gadm)
pop=read.table('gdam_cyberbullying.txt', header = T)
pop_s=pop[order(pop$Code)]
pop_s=pop[order(pop$Code),]
inter=c(0,22,35, 80,92, 123,131, 205, 261, 320, 411, 440, 510, 770, 1700)
pop_c=cut(pop_s$Y2013, breaks = inter)
gadm$pop=as.factor(pop_c)
col=rev(heat.colors(length(levels(gadm$pop))))
spplot(gadm, 'pop', col.regions=col, main="2013 school cyberbullying risks by region")
#3)지리적 데이터
rm(list=ls())
gadm=readRDS("KOR_adm0.rds")
plot(gadm)
gadm=readRDS("KOR_adm1.rds")
pop=read.table('gdam_cyberbullying.txt', header = T)
#시, 도에 대한 코드
pop_s=pop[order(pop$Code),]
#버즈량을 17개 구간으로 설정
inter=c(0,22,35, 80,92, 123,131, 205, 261, 320, 411, 440, 510, 770, 1700)
#2013년 항목을 17단계로 구분
pop_c=cut(pop_s$Y2013, breaks = inter)
gadm$pop=as.factor(pop_c)
col=rev(heat.colors(length(levels(gadm$pop))))
spplot(gadm, 'pop', col.regions=col, main="2013 school cyberbullying risks by region")
#막대그래프
rm(list=ls())
f=read.csv('cyberbullying_delequency.csv', sep=",", stringsAsFactors = F)
bp=barplot(f$Freqency, names.arg = f$delequency, main='delequency buzz tracking',
col=gray.colors(12), xlim=c(0,35000), cex.names = 0.5, col.main=1, font.main=2,
las=1, horiz = T)
#x축 변수(빈도), y축 변수, 제목, 흑백 12색, x축 범위, y축 글자크기, , ,축 라벨:수평,그래프 수평
text(y=bp, x=f$Freqency*1, pos=4, labels = paste(f$Freqency), 'case'), col='black', cex=0.4)
#x축 변수(빈도), y축 변수, 제목, 흑백 12색, x축 범위, y축 글자크기, , ,축 라벨:수평,그래프 수평
text(y=bp, x=f$Freqency*1, pos=4, labels = paste(f$Freqency,'case'), col='black', cex=0.4)
